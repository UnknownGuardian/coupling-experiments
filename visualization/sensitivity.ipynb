{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Inputs\r\n",
        "load = [100, 700] # load\r\n",
        "capacity = [1, 500] # capacity\r\n",
        "latency = [10, 80] # latency\r\n",
        "availability = [0, 1] # availability\r\n",
        "\r\n",
        "# Problem Definitions\r\n",
        "load2_problem = { # dropped capacity\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\r\n",
        "    'bounds': [ load, latency, availability, load],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'load2',\r\n",
        "        'models':[\"A\", \"B\", \"C\", \"D\"],\r\n",
        "        'independent':'loadFromX',\r\n",
        "        'dependent':[\r\n",
        "            'loadFromY', \r\n",
        "            'meanLatencyFromY',\r\n",
        "            \"meanResponseP1Latency\",\r\n",
        "            \"meanResponseP2Latency\",\r\n",
        "            \"meanResponseP3Latency\"\r\n",
        "        ]\r\n",
        "    }\r\n",
        "}\r\n",
        "load2_degraded_problem = {\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\r\n",
        "    'bounds': [ load, latency, availability, [800,1500]],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'load2',\r\n",
        "        'models':[\"A\", \"B\", \"C\", \"D\"],\r\n",
        "        'independent':'loadFromX',\r\n",
        "        'dependent':[\r\n",
        "            'loadFromY', \r\n",
        "            'meanLatencyFromY',\r\n",
        "            \"meanResponseP1Latency\",\r\n",
        "            \"meanResponseP2Latency\",\r\n",
        "            \"meanResponseP3Latency\"\r\n",
        "        ]\r\n",
        "    }\r\n",
        "}\r\n",
        "latency2_problem = {\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\r\n",
        "    'bounds': [ load, latency, availability, latency],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'latency2',\r\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\r\n",
        "        'independent':'meanLatencyFromZ',\r\n",
        "        'dependent':['meanLatencyFromY', 'meanAvailabilityFromY']\r\n",
        "    }\r\n",
        "}\r\n",
        "latencydegradation_problem = {\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\r\n",
        "    'bounds': [ load, latency, availability, [65, 120]],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'latency2',\r\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\r\n",
        "        'independent':'meanLatencyFromZ',\r\n",
        "        'dependent':['meanLatencyFromY', 'meanAvailabilityFromY']\r\n",
        "    }\r\n",
        "}\r\n",
        "availability2_problem = {\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newAvailability'],\r\n",
        "    'bounds': [ load, latency, availability, availability],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'availability2',\r\n",
        "        'models':[\"A\", \"H\", \"F\", \"I\", \"J\"],\r\n",
        "        'independent':'meanAvailabilityFromZ',\r\n",
        "        'dependent':['meanAvailabilityFromY', 'loadFromY']\r\n",
        "    }\r\n",
        "}\r\n",
        "capacity_problem = {\r\n",
        "    'num_vars': 5,\r\n",
        "    'names': ['xLoad', 'zCapacity', 'zLatency', 'zAvailability', 'newCapacity'],\r\n",
        "    'bounds': [ load, capacity, latency, availability, capacity],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'capacity',\r\n",
        "        'models':[\"A\", \"K\", \"L\"],\r\n",
        "        'independent':'zCapacity',\r\n",
        "        'dependent':['poolSize']\r\n",
        "    }\r\n",
        "}\r\n",
        "debug_problem = {\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\r\n",
        "    'bounds': [ load, latency, availability, latency],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'latency2',\r\n",
        "        'models':[\"G\"],\r\n",
        "        'independent':'meanLatencyFromZ',\r\n",
        "        'dependent':[\r\n",
        "            \"meanLatencyFromY\",\r\n",
        "            \"meanAvailabilityFromY\",\r\n",
        "            \"meanResponseGFastLatency\",\r\n",
        "            \"meanResponseGMediumLatency\",\r\n",
        "            \"meanResponseGSlowLatency\",\r\n",
        "            \"meanResponseGFastAvailability\",\r\n",
        "            \"meanResponseGMediumAvailability\",\r\n",
        "            \"meanResponseGSlowAvailability\"\r\n",
        "            ]\r\n",
        "    }\r\n",
        "}\r\n",
        "debug2_problem = { # dropped capacity\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\r\n",
        "    'bounds': [ load, latency, availability, load],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'load2',\r\n",
        "        'models':[\"C\", \"D\"],\r\n",
        "        'independent':'loadFromX',\r\n",
        "        'dependent':[\r\n",
        "            'loadFromY', \r\n",
        "            'meanLatencyFromY'\r\n",
        "            \"meanResponseP1Availability\",\r\n",
        "            \"meanResponseP2Availability\",\r\n",
        "            \"meanResponseP3Availability\"\r\n",
        "        ]\r\n",
        "    }\r\n",
        "}\r\n",
        "early_exit_try = { # dropped capacity\r\n",
        "    'num_vars': 4,\r\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\r\n",
        "    'bounds': [ load, latency, availability, load],\r\n",
        "    'simulation':{\r\n",
        "        'scenario':'latency2earlyexit',\r\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\r\n",
        "        'independent':'meanLatencyFromZ',\r\n",
        "        'dependent':[\r\n",
        "            \"meanLatencyFromY\",\r\n",
        "            \"meanAvailabilityFromY\",\r\n",
        "            \"meanResponseGFastLatency\",\r\n",
        "            \"meanResponseGMediumLatency\",\r\n",
        "            \"meanResponseGSlowLatency\",\r\n",
        "            \"meanResponseGFastAvailability\",\r\n",
        "            \"meanResponseGMediumAvailability\",\r\n",
        "            \"meanResponseGSlowAvailability\"\r\n",
        "            ]\r\n",
        "    }\r\n",
        "}\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from SALib.sample import saltelli\r\n",
        "import numpy as np\r\n",
        "from os import path\r\n",
        "import json\r\n",
        "\r\n",
        "# Model Inputs\r\n",
        "problem = debug_problem\r\n",
        "\r\n",
        "# Suggesting N = 1000\r\n",
        "param_values = saltelli.sample(problem, 128)\r\n",
        "\r\n",
        "sensitivity_path = path.join(\"..\", \"out\", \"sensitivity\")\r\n",
        "simulation_file_path = path.join(sensitivity_path, \"simulation.json\")\r\n",
        "param_file_path = path.join(sensitivity_path, \"param_values.txt\")\r\n",
        "\r\n",
        "print(\"saving\")\r\n",
        "print(\"\\t\", simulation_file_path)\r\n",
        "print(\"\\t\", param_file_path)\r\n",
        "\r\n",
        "np.savetxt(param_file_path, param_values, fmt='%1.4f')\r\n",
        "with open(simulation_file_path, 'w', encoding=\"utf-8\") as f:\r\n",
        "  json.dump(problem[\"simulation\"], f, ensure_ascii=False, indent=4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving\n",
            "\t ..\\out\\sensitivity\\simulation.json\n",
            "\t ..\\out\\sensitivity\\param_values.txt\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions To Ask\r\n",
        "- Do we want to have this model accept some combination of degradations at the same time or just one?\r\n",
        "    > Limit it to just one degradation at a time\r\n",
        "- Some scenarios have different architecture? Z has unlimited capacity in most scenarios.\r\n",
        "    > I think we need 1 \"model\" per architecture. Also means, we may need to consider different input distributions per scenario, since they would each be a situation (i.e. we care about latency in this one, so our latency distribution is wider, etc)\r\n",
        "- `TICK_DILATION` and output values? How does this affect our sensitivity if we report latency as actual latency scaled by the dilation value?\r\n",
        "    > I think we need to care about magnitude here, since the Y axis for sensitivity is this magnitude value.\r\n",
        "- We need to remove any random sampling from our models. They need to be deterministic\r\n",
        "    > Might be able to get away without determinisim in this case (otherwise, why do they run the models several times with same input)\r\n",
        "- We might need to cast inputs to integers (at least some of them)\r\n",
        "- We might need to not to do that nan to 0 thing at the end. This migtht mess up some M/S outputs\r\n",
        "\r\n",
        "## Run\r\n",
        "```\r\n",
        "npx ts-node src/sensitivity/index.ts\r\n",
        "```\r\n",
        "or with cluster:\r\n",
        "```\r\n",
        "npx ts-node src/sensitivity/cluster_start.ts\r\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Outputs from Time Series\r\n",
        "\"\"\" Some assistance from this article\r\n",
        "https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def crosscorr(datax, datay, lag=0, wrap=False):\r\n",
        "    \"\"\" Lag-N cross correlation. \r\n",
        "    Shifted data filled with NaNs \r\n",
        "    \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    lag : int, default 0\r\n",
        "    datax, datay : pandas.Series objects of equal length\r\n",
        "    Returns\r\n",
        "    ----------\r\n",
        "    crosscorr : float\r\n",
        "    \"\"\"\r\n",
        "    if wrap:\r\n",
        "        shiftedy = datay.shift(lag)\r\n",
        "        shiftedy.iloc[:lag] = datay.iloc[-lag:].values\r\n",
        "        return datax.corr(shiftedy)\r\n",
        "    else: \r\n",
        "        return datax.corr(datay.shift(lag))\r\n",
        "\r\n",
        "def time_series_crosscorr(d1, d2, window):\r\n",
        "    \"\"\"\r\n",
        "    Returns crosscorr, delay\r\n",
        "    \"\"\"\r\n",
        "    #rs = [crosscorr(d1, d2, lag) for lag in range(-window, 1)]\r\n",
        "    rs = [crosscorr(d1, d2, lag) for lag in range(-window, 1)]\r\n",
        "\r\n",
        "    # Early exit if no window\r\n",
        "    if window == 0:\r\n",
        "        #print(window, 0, rs)\r\n",
        "        return rs[0], 0\r\n",
        "    \r\n",
        "    maxCrossCorr = np.amax(rs)\r\n",
        "    maxCrossCorrIndex = np.argmax(rs)\r\n",
        "    minCrossCorr = np.amin(rs)\r\n",
        "    minCrossCorrIndex = np.argmin(rs)\r\n",
        "\r\n",
        "    actualCrossCorr = 0\r\n",
        "    actualCrossCorrIndex = 0\r\n",
        "\r\n",
        "    if maxCrossCorr > -1 * minCrossCorr:\r\n",
        "        actualCrossCorr = maxCrossCorr\r\n",
        "        actualCrossCorrIndex = maxCrossCorrIndex\r\n",
        "    else:\r\n",
        "        actualCrossCorr = minCrossCorr\r\n",
        "        actualCrossCorrIndex = minCrossCorrIndex\r\n",
        "\r\n",
        "    offset = -window + actualCrossCorrIndex\r\n",
        "    #print(window, actualCrossCorrIndex, rs)\r\n",
        "\r\n",
        "    return actualCrossCorr, offset\r\n",
        "\r\n",
        "def ending_n_mean(datay, n):\r\n",
        "    return np.mean(datay[-n:])\r\n",
        "\r\n",
        "def ending_n_slope(datay, n):\r\n",
        "    return (datay.iloc[-1] - datay.iloc[-n]) / n\r\n",
        "\r\n",
        "def start_end_mean_diff(datay, n):\r\n",
        "    return (datay.iloc[n] - datay.iloc[0]) / n - ending_n_slope(datay, n)\r\n",
        "\r\n",
        "def mean_n(datay, n, k):\r\n",
        "    start = n * k\r\n",
        "    end = start + n\r\n",
        "    \r\n",
        "    if len(datay) < start:\r\n",
        "        return np.nan\r\n",
        "    if(len(datay) < end):\r\n",
        "        return np.nan\r\n",
        "\r\n",
        "    return np.mean(datay[start:end])\r\n",
        "\r\n",
        "def slope_n(datay, n, k):\r\n",
        "    start = n * k\r\n",
        "    end = start + n\r\n",
        "    \r\n",
        "    if len(datay) <= start:\r\n",
        "        return np.nan\r\n",
        "    if(len(datay) <= end):\r\n",
        "        return np.nan\r\n",
        "\r\n",
        "    return (datay.iloc[end] - datay.iloc[start]) / n\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "metrics = [\r\n",
        "        #\"CrossCorr\",\r\n",
        "        #\"TimeSeriesCrossCorr\",\r\n",
        "        #\"TimeSeriesCrossCorrOffset\",\r\n",
        "        \"EndingNMean\"#,\r\n",
        "        #\"EndingNSlope\",\r\n",
        "        #\"StartEndMeanDiff\",\r\n",
        "        #\"M0\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\",\r\n",
        "        #\"S0\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\"\r\n",
        "    ]\r\n",
        "\r\n",
        "def get_dependent_name_from_output_index(dependents, output_index):\r\n",
        "    index = output_index // len(metrics)\r\n",
        "    return dependents[index]\r\n",
        "\r\n",
        "def get_output_name(index):\r\n",
        "    \"\"\"\r\n",
        "    Gets the corresponding metric name based on the index of the output\r\n",
        "    \"\"\"\r\n",
        "    return metrics[index % len(metrics)]\r\n",
        "\r\n",
        "def get_dependent_output(a, b):\r\n",
        "    \"\"\"\r\n",
        "    a: independent series\r\n",
        "    b: dependent series\r\n",
        "    \"\"\"\r\n",
        "    ending_n = 10\r\n",
        "\r\n",
        "    #c_a_b = crosscorr(a, b)\r\n",
        "    #tc_a_b, offset_a_b = time_series_crosscorr(a, b, 6)\r\n",
        "    em_a_b = ending_n_mean(b, ending_n)\r\n",
        "    #es_a_b = ending_n_slope(b, ending_n)\r\n",
        "    #sed_a_b = start_end_mean_diff(b, ending_n)\r\n",
        "\r\n",
        "    # selected to be 16, since that is when the degradation starts (8 seconds in, 16 measurements)\r\n",
        "    slice_size = 16\r\n",
        "    #m0 = mean_n(b, slice_size, 0) # initial warm up\r\n",
        "    #m1 = mean_n(b, slice_size, 1) # degradation - deg+8sec\r\n",
        "    #m2 = mean_n(b, slice_size, 2) # deg+8sec - deg+16sec\r\n",
        "    #m3 = mean_n(b, slice_size, 3)  # deg+16sec - deg+24sec\r\n",
        "    #m4 = mean_n(b, slice_size, 4)\r\n",
        "    #m5 = mean_n(b, slice_size, 5)\r\n",
        "    m_set = mean_n(b, 10, 2)# slice of 20 - 30, corresponding to seconds 10 - 15.\r\n",
        "\r\n",
        "    # backup to just use the last chunk if the 20-30 doesn't work\r\n",
        "    if np.isnan(m_set):\r\n",
        "        confirm = np.mean(b[20:])\r\n",
        "        print(\"USING BACKUP\", em_a_b, confirm, np.shape(b))\r\n",
        "        m_set = confirm\r\n",
        "        #m_set = em_a_b\r\n",
        "        #print(\"\\t\\tUsed backup mean\")\r\n",
        "\r\n",
        "    #s0 = slope_n(b, slice_size, 0)\r\n",
        "    #s1 = slope_n(b, slice_size, 1)\r\n",
        "    #s2 = slope_n(b, slice_size, 2)\r\n",
        "    #s3 = slope_n(b, slice_size, 3)\r\n",
        "    #s4 = slope_n(b, slice_size, 4)\r\n",
        "    #s5 = slope_n(b, slice_size, 5)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    return [m_set #em_a_b\r\n",
        "        #c_a_b, tc_a_b, offset_a_b, em_a_b, es_a_b, sed_a_b#, \r\n",
        "        #m0, m1, m2, m3, m4, m5, s0, s1, s2, s3, s4, s5\r\n",
        "    ]\r\n",
        "\r\n",
        "\r\n",
        "def get_output(series, independent, dependents, index):\r\n",
        "    independent_series = series[independent]\r\n",
        "\r\n",
        "    result = []\r\n",
        "    for d in dependents:\r\n",
        "        dependent_series = series[d]\r\n",
        "        result += get_dependent_output(independent_series, dependent_series)\r\n",
        "    \r\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Which simulation results to use, hardcoded because of the disconnect needing to run NodeJS in the previous simulation step\r\n",
        "time_series_path = path.join(\"..\", \"out\", \"sensitivity\", \"results-latency2-1630596225669\")\r\n",
        "\r\n",
        "with open(path.join(time_series_path, \"simulation.json\")) as json_file:\r\n",
        "    output_simulation_data = json.load(json_file)\r\n",
        "\r\n",
        "independent = output_simulation_data[\"independent\"]\r\n",
        "dependents = output_simulation_data[\"dependent\"]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "import glob\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "\r\n",
        "\r\n",
        "for model in output_simulation_data[\"models\"]:\r\n",
        "    print(\"Model:\", model)\r\n",
        "    time_series_model_output = path.join(time_series_path, model)\r\n",
        "    time_series_file_name_pattern = path.join(time_series_model_output, \"sim\", \"*.csv\")\r\n",
        "    all_time_series_csv_files = glob.glob(time_series_file_name_pattern)\r\n",
        "    print(\"\\tFound\", len(all_time_series_csv_files), \"files\")\r\n",
        "\r\n",
        "    # sort by the id so each row still matches the id of the time series\r\n",
        "    #print(all_time_series_csv_files)\r\n",
        "    #print([re.findall(\"(\\d+)\\-SteadyLatency\\.csv\",x) for x in all_time_series_csv_files])\r\n",
        "    sorted_time_series_files = sorted(\r\n",
        "        all_time_series_csv_files, \r\n",
        "        key=lambda x:float(re.findall(\"(\\d+)\\-SteadyLatency\\.csv\",x)[0])\r\n",
        "        )\r\n",
        "\r\n",
        "    #print(sorted_time_series_files)\r\n",
        "    #print([re.findall(\"sim\\\\\\\\\\w-(\\d+)\\-.*\\.csv\",x)[0] for x in all_time_series_csv_files])\r\n",
        "\r\n",
        "    time_series = [pd.read_csv(f) for f in sorted_time_series_files]\r\n",
        "    outputs = [get_output(s, independent, dependents, index) for index, s in enumerate(time_series)]\r\n",
        "\r\n",
        "    all_output_file_name = path.join(time_series_model_output, \"output_all_values.txt\")\r\n",
        "    np.savetxt(all_output_file_name, outputs, fmt='%1.8f')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\tFound 1280 files\n"
          ]
        }
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "from SALib.analyze import sobol\r\n",
        "import matplotlib.pyplot as plot\r\n",
        "from SALib.plotting.bar import plot as barplot\r\n",
        "\r\n",
        "\r\n",
        "def alert_nan_to_num(o):\r\n",
        "    f = np.isfinite(o).all()\r\n",
        "    if not f:\r\n",
        "        print(\"uh oh\")\r\n",
        "    return np.nan_to_num(o)\r\n",
        "\r\n",
        "# to print Numpy Arrays\r\n",
        "def default(obj):\r\n",
        "    if type(obj).__module__ == np.__name__:\r\n",
        "        if isinstance(obj, np.ndarray):\r\n",
        "            return obj.tolist()\r\n",
        "        else:\r\n",
        "            return obj.item()\r\n",
        "    raise TypeError('Unknown type:', type(obj))\r\n",
        "\r\n",
        "print(\"Assuming independent\", independent, \"and dependents\", dependents)\r\n",
        "\r\n",
        "for model in output_simulation_data[\"models\"]:\r\n",
        "    print(\"Model:\", model)\r\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\r\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\r\n",
        "\r\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\r\n",
        "    outputs = np.transpose(np.loadtxt(all_output_file_name, float))\r\n",
        "\r\n",
        "    converted = [alert_nan_to_num(o) for o in outputs]\r\n",
        "    analysis = [sobol.analyze(problem, o) for o in converted]\r\n",
        "\r\n",
        "    # save in JSON the FULL FILE including invalid NaN values for historical reasoning\r\n",
        "    analysis_output_file_name = path.join(time_series_model_output_path, \"analysis.json\")\r\n",
        "    with open(analysis_output_file_name, 'w', encoding=\"utf-8\") as f:\r\n",
        "        json.dump(analysis, f, ensure_ascii=False, indent=4, default=default)\r\n",
        "\r\n",
        "    # save only the Total Sensitivity in a csv\r\n",
        "    total_sensitivities = dict()\r\n",
        "    for d in dependents:\r\n",
        "        total_sensitivities[d] = dict()\r\n",
        "\r\n",
        "    for index, a in enumerate(analysis):\r\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\r\n",
        "        output_name = get_output_name(index)\r\n",
        "        total_sensitivities[dependent_name][output_name] = { \"st\":a[\"ST\"], \"conf\":a[\"ST_conf\"] }\r\n",
        "\r\n",
        "    sensitivity_output_file_name = path.join(time_series_model_output_path, \"sensitivity.json\")\r\n",
        "    with open(sensitivity_output_file_name, 'w', encoding=\"utf-8\") as f:\r\n",
        "        json.dump(total_sensitivities, f, ensure_ascii=False, indent=4, default=default)\r\n",
        "    #print(\"TOTAL_SENSITIVITIES\")\r\n",
        "    #print(total_sensitivities)\r\n",
        "\r\n",
        "    # give the labels some more room\r\n",
        "    # plot.gcf().subplots_adjust(bottom=0.15)\r\n",
        "\r\n",
        "    # print some quick graphs\r\n",
        "    for index, a in enumerate(analysis):\r\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\r\n",
        "        output_name = get_output_name(index)\r\n",
        "\r\n",
        "        #analysis_output_path = path.join(time_series_model_output_path, )\r\n",
        "        #with open(simulation_file_path, 'w', encoding=\"utf-8\") as f:\r\n",
        "        #    json.dump(problem[\"simulation\"], f, ensure_ascii=False, indent=4)\r\n",
        "\r\n",
        "        # Manually draw box plots\r\n",
        "        df = a.to_df()\r\n",
        "        fig, axes = plot.subplots(1, len(df))\r\n",
        "        fig.set_tight_layout(True)\r\n",
        "\r\n",
        "        fig.suptitle(f\"Sensitivity of {output_name}({index})\")\r\n",
        "        for idx, f in enumerate(df):\r\n",
        "            axes[idx] = barplot(f, ax=axes[idx])\r\n",
        "\r\n",
        "        img_path = path.join(time_series_model_output_path, f\"sen-{index}-{dependent_name}-{output_name}.png\")\r\n",
        "        plot.savefig(img_path, facecolor='white', transparent=False)\r\n",
        "        plot.close(fig)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Y = np.loadtxt(time_series_path + \"\\output_values.txt\", float)\r\n",
        "#Y = np.nan_to_num(Y)\r\n",
        "#Si = sobol.analyze(problem, Y)\r\n",
        "#print(Si)\r\n",
        "#print(Si['S1'])\r\n",
        "#print(Si['ST'])\r\n",
        "#Si.plot()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assuming independent meanLatencyFromZ and dependents ['meanLatencyFromY', 'meanAvailabilityFromY', 'meanResponseGFastLatency', 'meanResponseGMediumLatency', 'meanResponseGSlowLatency', 'meanResponseGFastAvailability', 'meanResponseGMediumAvailability', 'meanResponseGSlowAvailability']\n",
            "Model: G\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scatterplot Matrix"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plot\r\n",
        "from pandas.plotting import scatter_matrix\r\n",
        "\r\n",
        "for model in output_simulation_data[\"models\"]:\r\n",
        "    print(\"Model:\", model)\r\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\r\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\r\n",
        "\r\n",
        "\r\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\r\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\r\n",
        "\r\n",
        "    print(\"\\t\", np.shape(outputs))\r\n",
        "\r\n",
        "    num_columns = np.shape(outputs)[1]\r\n",
        "    num_outputs = len(metrics)\r\n",
        "    #print(num_columns, num_outputs)\r\n",
        "    for x in range(0, num_columns, num_outputs):\r\n",
        "        df = pd.DataFrame(outputs[:, x:x+num_outputs], columns=metrics)\r\n",
        "        scatter = scatter_matrix(df, alpha = 0.2, figsize = (5, 5), diagonal = 'hist')\r\n",
        "\r\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\r\n",
        "        scatter_file = path.join(time_series_model_output_path, f\"scatter_{d}.png\")\r\n",
        "        plot.savefig(scatter_file, facecolor='white', transparent=False)\r\n",
        "        plot.close(\"all\")\r\n",
        "\r\n",
        "\r\n",
        "    df = pd.DataFrame(outputs)\r\n",
        "    df.columns = [get_dependent_name_from_output_index(dependents, x) for x in range(0, num_columns, num_outputs)]\r\n",
        "    scatter = scatter_matrix(df, alpha = 0.2, figsize = (30, 30), diagonal = 'hist')\r\n",
        "    scatter_file = path.join(time_series_model_output_path, \"scatter.png\")\r\n",
        "    plot.savefig(scatter_file, facecolor='white', transparent=False)\r\n",
        "    plot.close(\"all\")\r\n",
        "    #print(\"\\tsaved image to \", scatter_file)\r\n",
        "    #break"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\t (640, 8)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "import pandas as pd\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\n",
        "\n",
        "    num_columns = np.shape(outputs)[1]\n",
        "    for x in range(0, num_columns):\n",
        "        df = pd.DataFrame(outputs[:,x])\n",
        "        #print(df)\n",
        "        #print(df.shape)\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\n",
        "        print(\"\\t\", d, df.var()[0])\n",
        "\n",
        "    #print(outputs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\t meanLatencyFromY 1.097469697070097\n",
            "\t meanAvailabilityFromY 0.06364061441330454\n",
            "\t meanResponseGFastLatency 0.04148599652937123\n",
            "\t meanResponseGMediumLatency 0.041539367070680615\n",
            "\t meanResponseGSlowLatency 0.0435585212692086\n",
            "\t meanResponseGFastAvailability 0.06439028950336974\n",
            "\t meanResponseGMediumAvailability 0.06493964322786282\n",
            "\t meanResponseGSlowAvailability 0.06543222552642405\n"
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "51246987221daf0abe1a6188d8696b481f0f1752001aee53b3542be4d9cf5e85"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
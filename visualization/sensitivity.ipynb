{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 150,
      "source": [
        "# Inputs\n",
        "load = [100, 700] # load\n",
        "capacity = [1, 500] # capacity\n",
        "latency = [10, 80] # latency\n",
        "availability = [0, 1] # availability\n",
        "\n",
        "# Problem Definitions\n",
        "load2_problem = { # dropped capacity\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\n",
        "    'bounds': [ load, latency, availability, load],\n",
        "    'simulation':{\n",
        "        'scenario':'load2',\n",
        "        'models':[\"A\", \"B\", \"C\", \"D\"],\n",
        "        'independent':'loadFromX',\n",
        "        'dependent':[\n",
        "            'loadFromY', \n",
        "            'meanLatencyFromY',\n",
        "            \"meanResponseP1Latency\",\n",
        "            \"meanResponseP2Latency\",\n",
        "            \"meanResponseP3Latency\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "load2_degraded_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\n",
        "    'bounds': [ load, latency, availability, [800,1500]],\n",
        "    'simulation':{\n",
        "        'scenario':'load2',\n",
        "        'models':[\"A\", \"B\", \"C\", \"D\"],\n",
        "        'independent':'loadFromX',\n",
        "        'dependent':[\n",
        "            'loadFromY', \n",
        "            'meanLatencyFromY',\n",
        "            \"meanResponseP1Latency\",\n",
        "            \"meanResponseP2Latency\",\n",
        "            \"meanResponseP3Latency\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "latency2_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\n",
        "    'bounds': [ load, latency, availability, latency],\n",
        "    'simulation':{\n",
        "        'scenario':'latency2',\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\n",
        "        'independent':'meanLatencyFromZ',\n",
        "        'dependent':['meanLatencyFromY', 'meanAvailabilityFromY']\n",
        "    }\n",
        "}\n",
        "latencydegradation_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\n",
        "    'bounds': [ load, latency, availability, [65, 120]],\n",
        "    'simulation':{\n",
        "        'scenario':'latency2',\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\n",
        "        'independent':'meanLatencyFromZ',\n",
        "        'dependent':['meanLatencyFromY', 'meanAvailabilityFromY']\n",
        "    }\n",
        "}\n",
        "availability2_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newAvailability'],\n",
        "    'bounds': [ load, latency, availability, availability],\n",
        "    'simulation':{\n",
        "        'scenario':'availability2',\n",
        "        'models':[\"A\", \"H\", \"F\", \"I\", \"J\"],\n",
        "        'independent':'meanAvailabilityFromZ',\n",
        "        'dependent':['meanAvailabilityFromY', 'loadFromY']\n",
        "    }\n",
        "}\n",
        "capacity_problem = {\n",
        "    'num_vars': 5,\n",
        "    'names': ['xLoad', 'zCapacity', 'zLatency', 'zAvailability', 'newCapacity'],\n",
        "    'bounds': [ load, capacity, latency, availability, capacity],\n",
        "    'simulation':{\n",
        "        'scenario':'capacity',\n",
        "        'models':[\"A\", \"K\", \"L\"],\n",
        "        'independent':'zCapacity',\n",
        "        'dependent':['poolSize']\n",
        "    }\n",
        "}\n",
        "debug_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLatency'],\n",
        "    'bounds': [ load, latency, availability, latency],\n",
        "    'simulation':{\n",
        "        'scenario':'latency2',\n",
        "        'models':[\"G\"],\n",
        "        'independent':'meanLatencyFromZ',\n",
        "        'dependent':[\n",
        "            \"meanLatencyFromY\",\n",
        "            \"meanAvailabilityFromY\",\n",
        "            \"meanResponseGFastLatency\",\n",
        "            \"meanResponseGMediumLatency\",\n",
        "            \"meanResponseGSlowLatency\",\n",
        "            \"meanResponseGFastAvailability\",\n",
        "            \"meanResponseGMediumAvailability\",\n",
        "            \"meanResponseGSlowAvailability\"\n",
        "            ]\n",
        "    }\n",
        "}\n",
        "debug2_problem = { # dropped capacity\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\n",
        "    'bounds': [ load, latency, availability, load],\n",
        "    'simulation':{\n",
        "        'scenario':'load2',\n",
        "        'models':[\"C\", \"D\"],\n",
        "        'independent':'loadFromX',\n",
        "        'dependent':[\n",
        "            'loadFromY', \n",
        "            'meanLatencyFromY'\n",
        "            \"meanResponseP1Availability\",\n",
        "            \"meanResponseP2Availability\",\n",
        "            \"meanResponseP3Availability\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "early_exit_try = { # dropped capacity\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\n",
        "    'bounds': [ load, latency, availability, load],\n",
        "    'simulation':{\n",
        "        'scenario':'latency2earlyexit',\n",
        "        'models':[\"A\", \"E\", \"F\", \"G\"],\n",
        "        'independent':'meanLatencyFromZ',\n",
        "        'dependent':[\n",
        "            \"meanLatencyFromY\",\n",
        "            \"meanAvailabilityFromY\",\n",
        "            \"meanResponseGFastLatency\",\n",
        "            \"meanResponseGMediumLatency\",\n",
        "            \"meanResponseGSlowLatency\",\n",
        "            \"meanResponseGFastAvailability\",\n",
        "            \"meanResponseGMediumAvailability\",\n",
        "            \"meanResponseGSlowAvailability\"\n",
        "            ]\n",
        "    }\n",
        "}\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "source": [
        "from SALib.sample import saltelli\n",
        "import numpy as np\n",
        "from os import path\n",
        "import json\n",
        "\n",
        "# Model Inputs\n",
        "problem = debug_problem\n",
        "\n",
        "# Suggesting N = 1000\n",
        "param_values = saltelli.sample(problem, 64)\n",
        "\n",
        "sensitivity_path = path.join(\"..\", \"out\", \"sensitivity\")\n",
        "simulation_file_path = path.join(sensitivity_path, \"simulation.json\")\n",
        "param_file_path = path.join(sensitivity_path, \"param_values.txt\")\n",
        "\n",
        "print(\"saving\")\n",
        "print(\"\\t\", simulation_file_path)\n",
        "print(\"\\t\", param_file_path)\n",
        "\n",
        "np.savetxt(param_file_path, param_values, fmt='%1.4f')\n",
        "with open(simulation_file_path, 'w', encoding=\"utf-8\") as f:\n",
        "  json.dump(problem[\"simulation\"], f, ensure_ascii=False, indent=4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving\n",
            "\t ../out/sensitivity/simulation.json\n",
            "\t ../out/sensitivity/param_values.txt\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions To Ask\n",
        "- Do we want to have this model accept some combination of degradations at the same time or just one?\n",
        "    > Limit it to just one degradation at a time\n",
        "- Some scenarios have different architecture? Z has unlimited capacity in most scenarios.\n",
        "    > I think we need 1 \"model\" per architecture. Also means, we may need to consider different input distributions per scenario, since they would each be a situation (i.e. we care about latency in this one, so our latency distribution is wider, etc)\n",
        "- `TICK_DILATION` and output values? How does this affect our sensitivity if we report latency as actual latency scaled by the dilation value?\n",
        "    > I think we need to care about magnitude here, since the Y axis for sensitivity is this magnitude value.\n",
        "- We need to remove any random sampling from our models. They need to be deterministic\n",
        "    > Might be able to get away without determinisim in this case (otherwise, why do they run the models several times with same input)\n",
        "- We might need to cast inputs to integers (at least some of them)\n",
        "- We might need to not to do that nan to 0 thing at the end. This migtht mess up some M/S outputs\n",
        "\n",
        "## Run\n",
        "```\n",
        "npx ts-node src/sensitivity/index.ts\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "# Outputs from Time Series\n",
        "\"\"\" Some assistance from this article\n",
        "https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9\n",
        "\"\"\"\n",
        "\n",
        "def crosscorr(datax, datay, lag=0, wrap=False):\n",
        "    \"\"\" Lag-N cross correlation. \n",
        "    Shifted data filled with NaNs \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lag : int, default 0\n",
        "    datax, datay : pandas.Series objects of equal length\n",
        "    Returns\n",
        "    ----------\n",
        "    crosscorr : float\n",
        "    \"\"\"\n",
        "    if wrap:\n",
        "        shiftedy = datay.shift(lag)\n",
        "        shiftedy.iloc[:lag] = datay.iloc[-lag:].values\n",
        "        return datax.corr(shiftedy)\n",
        "    else: \n",
        "        return datax.corr(datay.shift(lag))\n",
        "\n",
        "def time_series_crosscorr(d1, d2, window):\n",
        "    \"\"\"\n",
        "    Returns crosscorr, delay\n",
        "    \"\"\"\n",
        "    #rs = [crosscorr(d1, d2, lag) for lag in range(-window, 1)]\n",
        "    rs = [crosscorr(d1, d2, lag) for lag in range(-window, 1)]\n",
        "\n",
        "    # Early exit if no window\n",
        "    if window == 0:\n",
        "        #print(window, 0, rs)\n",
        "        return rs[0], 0\n",
        "    \n",
        "    maxCrossCorr = np.amax(rs)\n",
        "    maxCrossCorrIndex = np.argmax(rs)\n",
        "    minCrossCorr = np.amin(rs)\n",
        "    minCrossCorrIndex = np.argmin(rs)\n",
        "\n",
        "    actualCrossCorr = 0\n",
        "    actualCrossCorrIndex = 0\n",
        "\n",
        "    if maxCrossCorr > -1 * minCrossCorr:\n",
        "        actualCrossCorr = maxCrossCorr\n",
        "        actualCrossCorrIndex = maxCrossCorrIndex\n",
        "    else:\n",
        "        actualCrossCorr = minCrossCorr\n",
        "        actualCrossCorrIndex = minCrossCorrIndex\n",
        "\n",
        "    offset = -window + actualCrossCorrIndex\n",
        "    #print(window, actualCrossCorrIndex, rs)\n",
        "\n",
        "    return actualCrossCorr, offset\n",
        "\n",
        "def ending_n_mean(datay, n):\n",
        "    return np.mean(datay[-n:])\n",
        "\n",
        "def ending_n_slope(datay, n):\n",
        "    return (datay.iloc[-1] - datay.iloc[-n]) / n\n",
        "\n",
        "def start_end_mean_diff(datay, n):\n",
        "    return (datay.iloc[n] - datay.iloc[0]) / n - ending_n_slope(datay, n)\n",
        "\n",
        "def mean_n(datay, n, k):\n",
        "    start = n * k\n",
        "    end = start + n\n",
        "    \n",
        "    if len(datay) < start:\n",
        "        return np.nan\n",
        "    if(len(datay) < end):\n",
        "        return np.nan\n",
        "\n",
        "    return np.mean(datay[start:end])\n",
        "\n",
        "def slope_n(datay, n, k):\n",
        "    start = n * k\n",
        "    end = start + n\n",
        "    \n",
        "    if len(datay) <= start:\n",
        "        return np.nan\n",
        "    if(len(datay) <= end):\n",
        "        return np.nan\n",
        "\n",
        "    return (datay.iloc[end] - datay.iloc[start]) / n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "metrics = [\n",
        "        #\"CrossCorr\",\n",
        "        #\"TimeSeriesCrossCorr\",\n",
        "        #\"TimeSeriesCrossCorrOffset\",\n",
        "        \"EndingNMean\"#,\n",
        "        #\"EndingNSlope\",\n",
        "        #\"StartEndMeanDiff\",\n",
        "        #\"M0\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\",\n",
        "        #\"S0\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\"\n",
        "    ]\n",
        "\n",
        "def get_dependent_name_from_output_index(dependents, output_index):\n",
        "    index = output_index // len(metrics)\n",
        "    return dependents[index]\n",
        "\n",
        "def get_output_name(index):\n",
        "    \"\"\"\n",
        "    Gets the corresponding metric name based on the index of the output\n",
        "    \"\"\"\n",
        "    return metrics[index % len(metrics)]\n",
        "\n",
        "def get_dependent_output(a, b):\n",
        "    \"\"\"\n",
        "    a: independent series\n",
        "    b: dependent series\n",
        "    \"\"\"\n",
        "    ending_n = 10\n",
        "\n",
        "    #c_a_b = crosscorr(a, b)\n",
        "    #tc_a_b, offset_a_b = time_series_crosscorr(a, b, 6)\n",
        "    em_a_b = ending_n_mean(b, ending_n)\n",
        "    #es_a_b = ending_n_slope(b, ending_n)\n",
        "    #sed_a_b = start_end_mean_diff(b, ending_n)\n",
        "\n",
        "    # selected to be 16, since that is when the degradation starts (8 seconds in, 16 measurements)\n",
        "    slice_size = 16\n",
        "    #m0 = mean_n(b, slice_size, 0) # initial warm up\n",
        "    #m1 = mean_n(b, slice_size, 1) # degradation - deg+8sec\n",
        "    #m2 = mean_n(b, slice_size, 2) # deg+8sec - deg+16sec\n",
        "    #m3 = mean_n(b, slice_size, 3)  # deg+16sec - deg+24sec\n",
        "    #m4 = mean_n(b, slice_size, 4)\n",
        "    #m5 = mean_n(b, slice_size, 5)\n",
        "    m_set = mean_n(b, 10, 2)# slice of 20 - 30, corresponding to seconds 10 - 15.\n",
        "\n",
        "    # backup to just use the last chunk if the 20-30 doesn't work\n",
        "    if np.isnan(m_set):\n",
        "        confirm = np.mean(b[20:])\n",
        "        print(\"USING BACKUP\", em_a_b, confirm, np.shape(b))\n",
        "        m_set = confirm\n",
        "        #m_set = em_a_b\n",
        "        #print(\"\\t\\tUsed backup mean\")\n",
        "\n",
        "    #s0 = slope_n(b, slice_size, 0)\n",
        "    #s1 = slope_n(b, slice_size, 1)\n",
        "    #s2 = slope_n(b, slice_size, 2)\n",
        "    #s3 = slope_n(b, slice_size, 3)\n",
        "    #s4 = slope_n(b, slice_size, 4)\n",
        "    #s5 = slope_n(b, slice_size, 5)\n",
        "\n",
        "\n",
        "\n",
        "    return [m_set #em_a_b\n",
        "        #c_a_b, tc_a_b, offset_a_b, em_a_b, es_a_b, sed_a_b#, \n",
        "        #m0, m1, m2, m3, m4, m5, s0, s1, s2, s3, s4, s5\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_output(series, independent, dependents, index):\n",
        "    independent_series = series[independent]\n",
        "\n",
        "    result = []\n",
        "    for d in dependents:\n",
        "        dependent_series = series[d]\n",
        "        result += get_dependent_output(independent_series, dependent_series)\n",
        "    \n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "source": [
        "# Which simulation results to use, hardcoded because of the disconnect needing to run NodeJS in the previous simulation step\n",
        "time_series_path = path.join(\"..\", \"out\", \"sensitivity\", \"results-latency2-1630539954013\")\n",
        "\n",
        "with open(path.join(time_series_path, \"simulation.json\")) as json_file:\n",
        "    output_simulation_data = json.load(json_file)\n",
        "\n",
        "independent = output_simulation_data[\"independent\"]\n",
        "dependents = output_simulation_data[\"dependent\"]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output = path.join(time_series_path, model)\n",
        "    time_series_file_name_pattern = path.join(time_series_model_output, \"sim\", \"*.csv\")\n",
        "    all_time_series_csv_files = glob.glob(time_series_file_name_pattern)\n",
        "    print(\"\\tFound\", len(all_time_series_csv_files), \"files\")\n",
        "\n",
        "    # sort by the id so each row still matches the id of the time series\n",
        "    #print(all_time_series_csv_files)\n",
        "    #print([re.findall(\"(\\d+)\\-SteadyLatency\\.csv\",x) for x in all_time_series_csv_files])\n",
        "    sorted_time_series_files = sorted(\n",
        "        all_time_series_csv_files, \n",
        "        key=lambda x:float(re.findall(\"(\\d+)\\-SteadyLatency\\.csv\",x)[0])\n",
        "        )\n",
        "\n",
        "    #print(sorted_time_series_files)\n",
        "    #print([re.findall(\"sim\\\\\\\\\\w-(\\d+)\\-.*\\.csv\",x)[0] for x in all_time_series_csv_files])\n",
        "\n",
        "    time_series = [pd.read_csv(f) for f in sorted_time_series_files]\n",
        "    outputs = [get_output(s, independent, dependents, index) for index, s in enumerate(time_series)]\n",
        "\n",
        "    all_output_file_name = path.join(time_series_model_output, \"output_all_values.txt\")\n",
        "    np.savetxt(all_output_file_name, outputs, fmt='%1.8f')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\tFound 640 files\n"
          ]
        }
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "source": [
        "from SALib.analyze import sobol\n",
        "import matplotlib.pyplot as plot\n",
        "from SALib.plotting.bar import plot as barplot\n",
        "\n",
        "\n",
        "def alert_nan_to_num(o):\n",
        "    f = np.isfinite(o).all()\n",
        "    if not f:\n",
        "        print(\"uh oh\")\n",
        "    return np.nan_to_num(o)\n",
        "\n",
        "# to print Numpy Arrays\n",
        "def default(obj):\n",
        "    if type(obj).__module__ == np.__name__:\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return obj.item()\n",
        "    raise TypeError('Unknown type:', type(obj))\n",
        "\n",
        "print(\"Assuming independent\", independent, \"and dependents\", dependents)\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\n",
        "    outputs = np.transpose(np.loadtxt(all_output_file_name, float))\n",
        "\n",
        "    converted = [alert_nan_to_num(o) for o in outputs]\n",
        "    analysis = [sobol.analyze(problem, o) for o in converted]\n",
        "\n",
        "    # save in JSON the FULL FILE including invalid NaN values for historical reasoning\n",
        "    analysis_output_file_name = path.join(time_series_model_output_path, \"analysis.json\")\n",
        "    with open(analysis_output_file_name, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(analysis, f, ensure_ascii=False, indent=4, default=default)\n",
        "\n",
        "    # save only the Total Sensitivity in a csv\n",
        "    total_sensitivities = dict()\n",
        "    for d in dependents:\n",
        "        total_sensitivities[d] = dict()\n",
        "\n",
        "    for index, a in enumerate(analysis):\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\n",
        "        output_name = get_output_name(index)\n",
        "        total_sensitivities[dependent_name][output_name] = { \"st\":a[\"ST\"], \"conf\":a[\"ST_conf\"] }\n",
        "\n",
        "    sensitivity_output_file_name = path.join(time_series_model_output_path, \"sensitivity.json\")\n",
        "    with open(sensitivity_output_file_name, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(total_sensitivities, f, ensure_ascii=False, indent=4, default=default)\n",
        "    #print(\"TOTAL_SENSITIVITIES\")\n",
        "    #print(total_sensitivities)\n",
        "\n",
        "    # give the labels some more room\n",
        "    # plot.gcf().subplots_adjust(bottom=0.15)\n",
        "\n",
        "    # print some quick graphs\n",
        "    for index, a in enumerate(analysis):\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\n",
        "        output_name = get_output_name(index)\n",
        "\n",
        "        #analysis_output_path = path.join(time_series_model_output_path, )\n",
        "        #with open(simulation_file_path, 'w', encoding=\"utf-8\") as f:\n",
        "        #    json.dump(problem[\"simulation\"], f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Manually draw box plots\n",
        "        df = a.to_df()\n",
        "        fig, axes = plot.subplots(1, len(df))\n",
        "        fig.set_tight_layout(True)\n",
        "\n",
        "        fig.suptitle(f\"Sensitivity of {output_name}({index})\")\n",
        "        for idx, f in enumerate(df):\n",
        "            axes[idx] = barplot(f, ax=axes[idx])\n",
        "\n",
        "        img_path = path.join(time_series_model_output_path, f\"sen-{index}-{dependent_name}-{output_name}.png\")\n",
        "        plot.savefig(img_path, facecolor='white', transparent=False)\n",
        "        plot.close(fig)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#Y = np.loadtxt(time_series_path + \"\\output_values.txt\", float)\n",
        "#Y = np.nan_to_num(Y)\n",
        "#Si = sobol.analyze(problem, Y)\n",
        "#print(Si)\n",
        "#print(Si['S1'])\n",
        "#print(Si['ST'])\n",
        "#Si.plot()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assuming independent meanLatencyFromZ and dependents ['meanLatencyFromY', 'meanAvailabilityFromY', 'meanResponseGFastLatency', 'meanResponseGMediumLatency', 'meanResponseGSlowLatency', 'meanResponseGFastAvailability', 'meanResponseGMediumAvailability', 'meanResponseGSlowAvailability']\n",
            "Model: G\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scatterplot Matrix"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plot\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "\n",
        "\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\n",
        "\n",
        "    print(\"\\t\", np.shape(outputs))\n",
        "\n",
        "    num_columns = np.shape(outputs)[1]\n",
        "    num_outputs = len(metrics)\n",
        "    #print(num_columns, num_outputs)\n",
        "    for x in range(0, num_columns, num_outputs):\n",
        "        df = pd.DataFrame(outputs[:, x:x+num_outputs], columns=metrics)\n",
        "        scatter = scatter_matrix(df, alpha = 0.2, figsize = (5, 5), diagonal = 'hist')\n",
        "\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\n",
        "        scatter_file = path.join(time_series_model_output_path, f\"scatter_{d}.png\")\n",
        "        plot.savefig(scatter_file, facecolor='white', transparent=False)\n",
        "        plot.close(\"all\")\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(outputs)\n",
        "    df.columns = [get_dependent_name_from_output_index(dependents, x) for x in range(0, num_columns, num_outputs)]\n",
        "    scatter = scatter_matrix(df, alpha = 0.2, figsize = (30, 30), diagonal = 'hist')\n",
        "    scatter_file = path.join(time_series_model_output_path, \"scatter.png\")\n",
        "    plot.savefig(scatter_file, facecolor='white', transparent=False)\n",
        "    plot.close(\"all\")\n",
        "    #print(\"\\tsaved image to \", scatter_file)\n",
        "    #break"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\t (640, 8)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "import pandas as pd\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\n",
        "\n",
        "    num_columns = np.shape(outputs)[1]\n",
        "    for x in range(0, num_columns):\n",
        "        df = pd.DataFrame(outputs[:,x])\n",
        "        #print(df)\n",
        "        #print(df.shape)\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\n",
        "        print(\"\\t\", d, df.var()[0])\n",
        "\n",
        "    #print(outputs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: G\n",
            "\t meanLatencyFromY 1.097469697070097\n",
            "\t meanAvailabilityFromY 0.06364061441330454\n",
            "\t meanResponseGFastLatency 0.04148599652937123\n",
            "\t meanResponseGMediumLatency 0.041539367070680615\n",
            "\t meanResponseGSlowLatency 0.0435585212692086\n",
            "\t meanResponseGFastAvailability 0.06439028950336974\n",
            "\t meanResponseGMediumAvailability 0.06493964322786282\n",
            "\t meanResponseGSlowAvailability 0.06543222552642405\n"
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}